{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = [ ]\n",
    "for i in range(100):\n",
    "    x = np.random.uniform(-10.,10.)\n",
    "    eps = np.random.normal(0.,0.01)\n",
    "\n",
    "    y = 1.477 * x + 0.089 +eps\n",
    "\n",
    "    data.append([x,y])\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%% Compute Loss 计算误差\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_error_for_line_given_points(b,w,points):\n",
    "    # 根据当前的 w b参数计算均方差损失\n",
    "    totaError = 0\n",
    "    for i in range(0,len(points)): #循环迭代所有的点\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        #计算差的平方，并累加\n",
    "        totaError += (y - (w * x + b)) **2\n",
    "    #返回累加后的误差平均--- 均方差\n",
    "    return totaError / float(len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%% Compute Gradient and  update 计算梯度并更新\n"
    }
   },
   "outputs": [],
   "source": [
    "def step_gradient(b_current,w_current,points,learningRate):\n",
    "    b_grafient = 0\n",
    "    w_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0,len(points)):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        #分别求物产函数对b和w的倒数\n",
    "        b_grafient += (2/N) * ((w_current * x + b_current)-y)\n",
    "        w_gradient += (2/N) * ((w_current * x + b_current)-y)\n",
    "        #将倒数更新到误差函数  lr为 学习率（learningRate）\n",
    "    new_b = b_current - (learningRate * b_grafient)\n",
    "    new_w = w_current - (learningRate * w_gradient)\n",
    "    return [new_b,new_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%% 梯度更新\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(points,starting_b,starting_w,learning_rate,num_iterations):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "\n",
    "    for step in range(num_iterations):\n",
    "        b,w = step_gradient(b,w,np.array(points),learning_rate)\n",
    "        loss = compute_error_for_line_given_points(b,w,points)\n",
    "        if step % 50 == 0:\n",
    "            print(f\"iteration:{step},loss:{loss},w:{w},b:{b}\")\n",
    "        return [b,w] #返回最后一次的 b w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%main\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0,loss:74.05730802475637,w:0.04624147148368717,b:0.04624147148368717\n",
      "Final loss:74.05730802475637,w:0.04624147148368717,b:0.04624147148368717\n"
     ]
    }
   ],
   "source": [
    "#加载系统训练集\n",
    "learning_rate = 0.01 #学习率\n",
    "initial_b = 0\n",
    "initial_w = 0\n",
    "num_iterations = 1000\n",
    "[b,w] = gradient_descent(data,initial_b,initial_w,learning_rate,num_iterations)\n",
    "loss = compute_error_for_line_given_points(b,w,data)\n",
    "print(f\"Final loss:{loss},w:{w},b:{b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
